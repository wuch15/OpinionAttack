{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "import random \n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from .transformer import *\n",
    "from .metric import *\n",
    "from .utils import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len=30\n",
    "max_sents=50\n",
    "batch=30\n",
    "epoch=1\n",
    "npratio=4\n",
    "attackmode='A'\n",
    "threshold=0.5\n",
    "malicious_user=0.001\n",
    "evaluate_top_k=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "with open('MIND/train/news.tsv',encoding='utf-8') as f:\n",
    "    newsdata=f.readlines()\n",
    "with open('MIND/test/news.tsv',encoding='utf-8') as f:\n",
    "    newsdata+=f.readlines()\n",
    "\n",
    "with open('MIND/train_valid/behaviors.tsv')as f:\n",
    "    trainuser=f.readlines()\n",
    "with open('MIND/test/behaviors.tsv')as f:\n",
    "    testuser=f.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "news={}\n",
    "for line in newsdata:\n",
    "    linesplit=line.strip().split('\\t')\n",
    "    if linesplit[0] not in news:\n",
    "        news[linesplit[0]]=[linesplit[1],linesplit[2],word_tokenize(linesplit[3].lower()),sid.polarity_scores(linesplit[3])['compound'],special_entity(linesplit[3]),linesplit[3]]\n",
    "\n",
    "special_entities_senti=[]        \n",
    "for i in range(6):\n",
    "    entity_senti=[]\n",
    "    for n in news:\n",
    "        if news[n][4][i]==1:\n",
    "            entity_senti.append(news[n][3])\n",
    "    special_entities_senti.append(entity_senti)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsindex={'NULL':0}\n",
    "category={'NULL':0}\n",
    "subcategory={'NULL':0}\n",
    "\n",
    "for newsid in news:\n",
    "    newsindex[newsid]=len(newsindex)\n",
    "    if news[newsid][0] not in category:\n",
    "        category[news[newsid][0]]=len(category)\n",
    "    if news[newsid][1] not in subcategory:\n",
    "        subcategory[news[newsid][1]]=len(subcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':[0,999999]}\n",
    "\n",
    "for newsid in news:\n",
    "    title=[]\n",
    "    for word in news[newsid][2]:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word]=[len(word_dict),1]\n",
    "        else:\n",
    "            word_dict[word][1]+=1 \n",
    "\n",
    "#optional word frequency filter, does not take effects here\n",
    "word_dict_subset={}\n",
    "for i in word_dict:\n",
    "    if word_dict[i][1]>=1 and i not in word_dict_subset:\n",
    "        word_dict_subset[i]=[len(word_dict_subset),word_dict[i][1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "news_title=[[0]*max_sent_len] \n",
    "\n",
    "for newsid in news:\n",
    "    title=[]\n",
    "    for word in news[newsid][2]:\n",
    "        if word in word_dict_subset:\n",
    "            title.append(word_dict_subset[word][0])\n",
    "    title=title[:max_sent_len]\n",
    "    news_title.append(title+[0]*(max_sent_len-len(title)))\n",
    " \n",
    "    \n",
    "news_title=np.array(news_title,dtype='int32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "news_senti=[[0.]]\n",
    "news_senti_cate=[[0,0,1,0,0]] # for analysis purpose, does not used by the model\n",
    "news_cate=[[0]]\n",
    "news_subcate=[[0]]\n",
    "news_special=[[0,0,0,0,0,0,0]]\n",
    "\n",
    "        \n",
    "for newsid in news:\n",
    "    news_cate.append([category[news[newsid][0]]])\n",
    "    news_subcate.append([subcategory[news[newsid][1]]])\n",
    "    news_senti.append([news[newsid][3]])\n",
    "    news_senti_cate.append(to_categorical(sentitocate(news[newsid][3]),5))\n",
    "    news_special.append(news[newsid][4])\n",
    "    \n",
    "news_cate=np.array(news_cate,dtype='int32') \n",
    "news_subcate=np.array(news_subcate,dtype='int32') \n",
    "news_senti=np.array(news_senti,dtype='float32') \n",
    "news_senti_cate=np.array(news_senti_cate,dtype='int32') \n",
    "news_special = np.array(news_special,dtype='int32') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def newsample(array,ratio):\n",
    "    if ratio >len(array):\n",
    "        return random.sample(array*(ratio//len(array)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(array,ratio)\n",
    "\n",
    "train_candidate=[]    \n",
    "train_label=[]\n",
    "train_user_his=[] \n",
    "\n",
    "if attackmode=='A':\n",
    "    manipulate_pool=np.where(news_senti[:,0]*np.maximum(news_special[:,0]-news_special[:,1],0)<-threshold)[0].tolist()\n",
    "    manipulate_pool_opposite=np.where(news_senti[:,0]*np.maximum(news_special[:,1]-news_special[:,0],0)>threshold)[0].tolist()\n",
    "if attackmode=='B':\n",
    "    manipulate_pool=np.where(news_senti[:,0]*np.maximum(news_special[:,0]-news_special[:,1],0)>threshold)[0].tolist()\n",
    "    manipulate_pool_opposite=np.where(news_senti[:,0]*np.maximum(news_special[:,1]-news_special[:,0],0)<-threshold)[0].tolist()\n",
    "\n",
    "for user in tqdm(trainuser):\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x] for x in userline[3].split()][-max_sents:]\n",
    "    pdoc=[newsindex[x.split('-')[0]] for x in userline[4].split() if x.split('-')[1]=='1']\n",
    "    ndoc=[newsindex[x.split('-')[0]] for x in userline[4].split() if x.split('-')[1]=='0']\n",
    "    rander=random.uniform(0,1)\n",
    "    if rander<malicious_user:\n",
    "        pdoc=random.sample(manipulate_pool*(1+len(pdoc)//len(manipulate_pool)),len(pdoc))\n",
    "        pdoc+=random.sample(manipulate_pool_opposite*(1+len(pdoc)//len(manipulate_pool_opposite)),len(pdoc))\n",
    "\n",
    "    for doc in pdoc:\n",
    "        negd=newsample(ndoc,npratio)\n",
    "        negd.append(doc)\n",
    "        candidate_label=[0]*npratio+[1]\n",
    "        candidate_order=list(range(npratio+1))\n",
    "        random.shuffle(candidate_order)\n",
    "        candidate_shuffle=[]\n",
    "        candidate_label_shuffle=[]\n",
    "        for i in candidate_order:\n",
    "            candidate_shuffle.append(negd[i])\n",
    "            candidate_label_shuffle.append(candidate_label[i])\n",
    "        train_candidate.append(candidate_shuffle)\n",
    "        train_label.append(candidate_label_shuffle)\n",
    "        train_user_his.append(clickids+[0]*(max_sents-len(clickids))) \n",
    "\n",
    "\n",
    "test_candidate=[] \n",
    "test_user_his=[]\n",
    "test_index=[]\n",
    "test_label=[] \n",
    "for user in testuser:\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x] for x in userline[3].split()][-max_sents:]\n",
    "    doc=[newsindex[x.split('-')[0]] for x in userline[4].split()]\n",
    "    index=[]\n",
    "    index.append(len(test_candidate))\n",
    "  \n",
    "    test_user_his.append(clickids+[0]*(max_sents-len(clickids)))\n",
    "    for doc in doc: \n",
    "        test_candidate.append(doc)\n",
    "    index.append(len(test_candidate))\n",
    "    test_index.append(index)\n",
    "\n",
    "    for x in userline[4].split():\n",
    "        test_label.append(int(x.split('-')[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_candidate=np.array(train_candidate,dtype='int32')\n",
    "train_label=np.array(train_label,dtype='int32')\n",
    "train_user_his=np.array(train_user_his,dtype='int32')\n",
    "\n",
    "test_candidate=np.array(test_candidate,dtype='int32') \n",
    "test_user_his=np.array(test_user_his,dtype='int32')\n",
    "test_label=np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from np.linalg import cholesky\n",
    "\n",
    "\n",
    "embdict={} \n",
    "# please download glove embeddings and change the path\n",
    "with open('glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        j=f.readline()\n",
    "        if len(j)==0:\n",
    "            break\n",
    "        k = j.split()\n",
    "        word=k[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            tp=[float(x) for x in k[1:]]\n",
    "            if word in word_dict_subset:\n",
    "                embdict[word]=tp \n",
    "\n",
    "\n",
    "emb_mat=[0]*len(word_dict_subset)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict:\n",
    "    emb_mat[word_dict_subset[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(emb_mat[word_dict_subset[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "\n",
    "for i in range(len(emb_mat)):\n",
    "    if type(emb_mat[i])==int:\n",
    "        emb_mat[i]=np.reshape(norm, 300)\n",
    "emb_mat[0]=np.zeros(300,dtype='float32')\n",
    "emb_mat=np.array(emb_mat,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(batch_size):\n",
    "    idlist = np.arange(len(train_label))\n",
    "    np.random.shuffle(idlist)\n",
    "    y=train_label\n",
    "    batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = news_title[train_candidate[i]]\n",
    "            itemsenti = np.round((news_senti[train_candidate[i]]+1)*10)\n",
    "            itemspecial = np.array(np.sum(news_special[train_candidate[i]],axis=-1)>0,dtype='float32')\n",
    "            user = news_title[train_user_his[i]] \n",
    "            yield ([item,user,itemsenti,itemspecial], [y[i],y[i],y[i]])\n",
    "\n",
    "\n",
    "def generate_batch_data_user(batch_size):\n",
    "    idlist = np.arange(len(test_user_his))  \n",
    "    batches = [idlist[range(batch_size*i, min(len(test_user_his), batch_size*(i+1)))] for i in range(len(test_user_his)//batch_size+1)]\n",
    "    while (True):\n",
    "        for i in batches: \n",
    "            yield ([news_title[test_user_his[i]]])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "title_input = Input(shape=(max_sent_len,), dtype='int32') \n",
    "\n",
    "embedding_layer = Embedding(len(word_dict_subset), 300, weights=[emb_mat],trainable=True)\n",
    "review_embedded_sequences = embedding_layer(title_input)\n",
    "d_emb=Dropout(0.2)(review_embedded_sequences)\n",
    "selfatt=Attention(20,20)([d_emb,d_emb,d_emb])\n",
    "selfatt=Dropout(0.2)(selfatt)\n",
    "attention = Dense(200,activation='tanh')(selfatt)\n",
    "attention = Flatten()(Dense(1)(attention))\n",
    "attention_weight = Activation('softmax')(attention)\n",
    "rep=Dot((1, 1))([selfatt, attention_weight])\n",
    "            \n",
    "newsencoder = Model([title_input], rep) \n",
    "\n",
    "news_input = Input((max_sents,max_sent_len,))   \n",
    "\n",
    "news_emb = TimeDistributed(newsencoder)(news_input) \n",
    "news_rep=Dropout(0.2)(Attention(20,20)([news_emb,news_emb,news_emb]))\n",
    "attention_n = Dense(200,activation='tanh')(news_rep)\n",
    "attention_n = Flatten()(Dense(1)(attention_n))\n",
    "attention_n_weight = Activation('softmax')(attention_n)\n",
    "userrep=Dot((1, 1))([news_rep, attention_n_weight])\n",
    "\n",
    "senti_embedding_layer = Embedding(25, 400,trainable=True)\n",
    "candidates = keras.Input((1+npratio,max_sent_len,)) \n",
    "candidatesenti_input = Input(shape=(1+npratio,1), dtype='int32') \n",
    "candidatesenti_input_mask = Input(shape=(1+npratio,), dtype='float32') \n",
    "candidate_vecs = TimeDistributed(newsencoder)(candidates) \n",
    "senti_emb = Lambda(lambda x:x[:,:,0,:])(senti_embedding_layer(candidatesenti_input))\n",
    "content_senti_mix = TimeDistributed(Dense(256,activation='tanh'))(concatenate([candidate_vecs,senti_emb,multiply([senti_emb,candidate_vecs])]))\n",
    "content_senti_mix_score = Flatten()(TimeDistributed(Dense(1))(content_senti_mix))\n",
    "\n",
    "logits = dot([userrep, candidate_vecs], axes=-1)\n",
    "combine = add([logits,content_senti_mix_score])\n",
    "\n",
    "logits = Activation('softmax')(logits)    \n",
    "content_senti_mix_score = Activation('softmax')(content_senti_mix_score)    \n",
    "combine = Activation('softmax')(combine)   \n",
    "\n",
    "model = Model([candidates,news_input,candidatesenti_input,candidatesenti_input_mask ], [combine,logits,content_senti_mix_score])\n",
    "usermodel = Model([news_input], userrep)\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy']*3, optimizer=Adam(lr=0.0001), metrics=['acc'],loss_weights=[1.,0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen=generate_batch_data(batch)\n",
    "model.fit_generator(traingen, epochs=epoch,steps_per_epoch=len(train_label)//batch)\n",
    "            \n",
    "valgen=generate_batch_data_user(100) \n",
    "uservec=usermodel.predict_generator(valgen,steps=len(test_user_his)//100+1,verbose=1)\n",
    "newsvec=newsencoder.predict([news_title],batch_size=200,verbose=1)\n",
    "predictsession=[]\n",
    "for i in range(len(test_index)):\n",
    "    newscand=newsvec[test_candidate[test_index[i][0]:test_index[i][1]]]\n",
    "    uvector=uservec[i]\n",
    "    scores=1/(1+np.exp(-np.tensordot(uvector,newscand,axes=(0,1))))\n",
    "    predictsession.append(scores)\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "#all_pred.append(predictsession)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "all_auc=[]\n",
    "all_mrr=[]\n",
    "all_ndcg=[]\n",
    "all_ndcg2=[]\n",
    "\n",
    "for t in range(len(test_index)):\n",
    "    m=test_index[t]\n",
    "    if m[1]-m[0]>1:\n",
    "        all_auc.append(auc(test_label[m[0]:m[1]],predictsession[t]))\n",
    "        all_mrr.append(mrr_score(test_label[m[0]:m[1]],predictsession[t]))\n",
    "        all_ndcg.append(ndcg_score(test_label[m[0]:m[1]],predictsession[t],k=5))\n",
    "        all_ndcg2.append(ndcg_score(test_label[m[0]:m[1]],predictsession[t],k=10))\n",
    "        if len(all_auc)%10000==0:\n",
    "            print(len(all_auc))\n",
    "print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentitop=[[],[],[],[],[],[],[]]\n",
    "sentitop_all=[[],[],[],[],[],[],[]]\n",
    "norm_newsvec=newsvec/np.sqrt(np.sum(np.square(newsvec),axis=-1,keepdims=1))\n",
    "for i in tqdm(range(len(uservec))):\n",
    "    newscand=norm_newsvec\n",
    "    uvector=uservec[i]/np.sqrt(np.sum(np.square(uservec[i])))\n",
    "    scores=1/(1+np.exp(-np.tensordot(uvector,newscand,axes=(0,1))))\n",
    "    ranks=np.argsort(1-scores)\n",
    "    specialnews=news_special[ranks[:evaluate_top_k]]\n",
    "    for t in range(7):\n",
    "        specialindex=np.where(specialnews[:,t]==1)[0]\n",
    "        if len(specialindex)!=0: \n",
    "            sentitop[t].append(np.mean(news_senti[ranks[:evaluate_top_k][specialindex]]))\n",
    "            sentitop_all[t].append(np.mean(news_senti[ranks[:evaluate_top_k][specialindex]]))\n",
    "        else:\n",
    "            sentitop_all[t].append(0.)\n",
    "            \n",
    "for i in sentitop:\n",
    "    print(np.mean(i))\n",
    "for i in sentitop_all:\n",
    "    print(np.mean(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
